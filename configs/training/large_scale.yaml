# =============================================================================
# Large Scale Training Configuration
# =============================================================================
# Settings for multi-GPU/multi-node training on H100 clusters.
# Use with: python scripts/train.py training=large_scale model=dit_l2

# Optimization
learning_rate: 1e-4
weight_decay: 0.05
warmup_steps: 5000
max_steps: 500000
grad_clip: 1.0
grad_accumulation_steps: 2

# Batch Settings (per GPU)
batch_size: 16
num_workers: 8

# Effective batch size = batch_size * grad_accum * world_size
# With 8 GPUs: 16 * 2 * 8 = 256

# EMA Settings
ema_decay: 0.9999
ema_warmup_steps: 5000

# Drifting Loss Settings
drifting:
  temperatures: [0.02, 0.05, 0.2]
  normalize_features: true
  normalize_drift: true
  n_pos_samples: 64
  n_neg_samples: 64

# Mixed Precision (BF16 optimal for H100)
use_amp: true
amp_dtype: 'bfloat16'

# Distributed Training
use_fsdp: true
fsdp_shard_size: 100000000

# Checkpointing
checkpoint_dir: '${OUTPUT_DIR}/checkpoints'
save_every_n_steps: 10000
keep_last_n_checkpoints: 3

# Logging
log_every_n_steps: 50
eval_every_n_steps: 5000

# WandB
wandb:
  project: 'drifting-vla'
  entity: null
  mode: 'offline'  # Sync after training
  tags: ['large-scale', 'h100']
  log_images: true
  log_videos: false
  image_log_freq: 5000

