# =============================================================================
# DiT-L/2 Model Configuration
# =============================================================================
# Large model size with 24 layers, 1024 hidden dim, 16 heads.
# For larger datasets and higher performance requirements.

name: 'dit_l2'

# Vision Encoder
vision:
  model_name: 'dinov2_vitl14'
  image_size: 224
  patch_size: 14
  hidden_dim: 1024
  freeze: true
  output_tokens: 'all'

# Language Encoder  
language:
  model_name: 'openai/clip-vit-large-patch14'
  hidden_dim: 1024
  max_length: 77
  freeze: true
  pooling: 'eos'

# Fusion Module
fusion:
  hidden_dim: 1024
  num_heads: 16
  num_layers: 2
  dropout: 0.0
  use_rope: true
  qk_norm: true

# DiT Transformer
transformer:
  hidden_dim: 1024
  num_heads: 16
  num_layers: 24
  mlp_ratio: 4.0
  dropout: 0.0
  use_rope: true
  use_qk_norm: true
  use_swiglu: true
  use_flash_attn: true
  conditioning_dim: 1024

# Action Decoder
# Note: RLBench uses 8-dim actions (3 pos + 4 quat + 1 gripper)
action_decoder:
  hidden_dim: 1024
  action_horizon: 16
  position_dim: 3
  rotation_dim: 4
  gripper_dim: 1
  num_layers: 2
  dropout: 0.0
  action_scale: 1.0

# Global Settings
hidden_dim: 1024
action_horizon: 16
noise_dim: 64
cfg_scale_range: [1.0, 4.0]
cfg_dropout: 0.1


