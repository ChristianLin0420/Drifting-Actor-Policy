# =============================================================================
# DiT-B/2 Model Configuration
# =============================================================================
# Base model size with 12 layers, 768 hidden dim, 12 heads.
# Suitable for initial experiments and smaller datasets.

name: 'dit_b2'

# Vision Encoder
vision:
  model_name: 'dinov2_vitl14'
  image_size: 224
  patch_size: 14
  hidden_dim: 1024
  freeze: true
  output_tokens: 'all'

# Language Encoder
language:
  model_name: 'openai/clip-vit-large-patch14'
  hidden_dim: 1024
  max_length: 77
  freeze: true
  pooling: 'eos'

# Fusion Module
fusion:
  hidden_dim: 768
  num_heads: 12
  num_layers: 2
  dropout: 0.0
  use_rope: true
  qk_norm: true

# DiT Transformer
transformer:
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  mlp_ratio: 4.0
  dropout: 0.0
  use_rope: true
  use_qk_norm: true
  use_swiglu: true
  use_flash_attn: true
  conditioning_dim: 768

# Action Decoder
# Note: RLBench uses 8-dim actions (3 pos + 4 quat + 1 gripper)
action_decoder:
  hidden_dim: 768
  action_horizon: 16
  position_dim: 3
  rotation_dim: 4
  gripper_dim: 1
  num_layers: 2
  dropout: 0.0
  action_scale: 1.0

# Global Settings
hidden_dim: 768
action_horizon: 16
noise_dim: 64
cfg_scale_range: [1.0, 4.0]
cfg_dropout: 0.1


