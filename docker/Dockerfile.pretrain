# =============================================================================
# Drifting-VLA Pre-Training Docker Image
# =============================================================================
# Vision-encoder-only architecture (Pi0 style) — ViT encoder + text embeddings,
# LLM decoder is never loaded or run.
#
# Build:
#   docker build -f docker/Dockerfile.pretrain -t drifting-vla:pretrain .
#
# Run (interactive development — code synced from host):
#   docker run --gpus all --ipc=host --ulimit memlock=-1 \
#       -v $(pwd):/workspace \
#       -v $(pwd)/data:/workspace/data \
#       -e WANDB_API_KEY=$WANDB_API_KEY \
#       drifting-vla:pretrain bash
#
# Run (data preparation, 1% smoke-test):
#   docker run --gpus all \
#       -v $(pwd)/data:/workspace/data \
#       drifting-vla:pretrain \
#       python scripts/prepare_data.py --all --data-fraction 0.01
#
# Run (training, 8×A40/H100):
#   docker run --gpus all --ipc=host --ulimit memlock=-1 \
#       -v $(pwd):/workspace \
#       -v $(pwd)/data:/workspace/data \
#       -e WANDB_API_KEY=$WANDB_API_KEY \
#       drifting-vla:pretrain \
#       torchrun --nproc_per_node=8 scripts/train.py \
#           --datasets aloha bc_z behavior1k_t0000 cmu_stretch dexgraspnet \
#               dexwild nyu_franka rlbench stanford_hydra taco_play utaustin_mutex \
#           --episodes-root ./data/episodes \
#           --batch-size 32 --grad-accumulation 2 \
#           --max-steps 10000 --lr 1e-4 \
#           --vlm-mode lora --lora-r 16 --vlm-lr-scale 0.1 \
#           --loss-type hybrid --model-size base \
#           --wandb-mode online --wandb-project drifting-vla
#
# Notes:
#   - Data is volume-mounted, NEVER baked into the image
#   - WandB auto-login via WANDB_API_KEY env var
#   - Entrypoint handles WandB login + exec
# =============================================================================

FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# =============================================================================
# System Dependencies
# =============================================================================
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Build tools
    build-essential \
    cmake \
    ninja-build \
    git \
    wget \
    curl \
    unzip \
    # Python
    python3.10 \
    python3.10-dev \
    python3.10-venv \
    python3-pip \
    # Graphics and rendering
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libglew-dev \
    libosmesa6-dev \
    # X11 for headless rendering
    xvfb \
    x11-utils \
    # HDF5 and compression
    libhdf5-dev \
    liblz4-dev \
    libzstd-dev \
    # Networking
    openssh-client \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.10 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 \
    && python -m pip install --upgrade pip setuptools wheel

# =============================================================================
# PyTorch + CUDA 12.1
# =============================================================================
RUN pip install --no-cache-dir \
    torch==2.2.0 \
    torchvision==0.17.0 \
    torchaudio==2.2.0 \
    --index-url https://download.pytorch.org/whl/cu121

# =============================================================================
# Flash Attention 2 (optional, for H100/B200)
# =============================================================================
RUN pip install --no-cache-dir flash-attn==2.5.0 --no-build-isolation || true

# =============================================================================
# VLM + Fine-Tuning
# =============================================================================
RUN pip install --no-cache-dir \
    "transformers>=4.57" \
    "accelerate>=0.27.0" \
    "peft>=0.8.0" \
    "safetensors>=0.4.2" \
    "huggingface-hub>=0.21.0" \
    qwen-vl-utils

# =============================================================================
# Data Loading (LeRobot API + HDF5)
# =============================================================================
RUN pip install --no-cache-dir \
    lerobot \
    "datasets>=2.18.0" \
    "h5py>=3.10.0"

# =============================================================================
# Scientific Computing
# =============================================================================
RUN pip install --no-cache-dir \
    "numpy>=1.26" \
    "scipy>=1.12" \
    "scikit-learn>=1.4"

# =============================================================================
# Image / Video Processing
# =============================================================================
RUN pip install --no-cache-dir \
    "opencv-python-headless>=4.9" \
    "Pillow>=10.2"

# =============================================================================
# Visualization + Logging
# =============================================================================
RUN pip install --no-cache-dir \
    "wandb>=0.16" \
    "matplotlib>=3.8"

# =============================================================================
# Utilities
# =============================================================================
RUN pip install --no-cache-dir \
    "einops>=0.7" \
    "tqdm>=4.66" \
    "rich>=13.7" \
    "open3d>=0.18"

# =============================================================================
# Environment Variables
# =============================================================================
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"
ENV WANDB_MODE=online
ENV WANDB_SILENT=true
# Prevent file locking issues with HDF5 in distributed training
ENV HDF5_USE_FILE_LOCKING=FALSE
# Prevent NCCL P2P hangs on PCIe GPU topologies (e.g., A40)
ENV NCCL_P2P_DISABLE=1
# Optimize CUDA memory allocation
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

WORKDIR /workspace

# =============================================================================
# Project Source (explicit copy — data/ is NEVER included)
# =============================================================================
COPY setup.py pyproject.toml README.md LICENSE /workspace/
COPY drifting_vla/ /workspace/drifting_vla/
COPY scripts/ /workspace/scripts/
COPY configs/ /workspace/configs/
COPY requirements/ /workspace/requirements/
COPY docker/ /workspace/docker/

RUN cd /workspace && pip install --no-cache-dir -e .
RUN pip install -U huggingface_hub

# Build-time safety: fail if data/ leaked into context
RUN test ! -d /workspace/data

# =============================================================================
# WandB Configuration
# Auto-login: set WANDB_API_KEY at runtime via:
#   docker run -e WANDB_API_KEY=your_key ...
# =============================================================================
# Fix git ownership warning for WandB
RUN git config --global --add safe.directory /workspace

# =============================================================================
# Health Check: verify all key imports
# =============================================================================
RUN python -c "\
from drifting_vla.models.vlm_backbone import VLMBackbone, VLMConfig; \
from drifting_vla.models.drifting_vla import DriftingVLA, DriftingVLAConfig; \
from drifting_vla.data.episode_dataset import EpisodeHDF5Dataset; \
from drifting_vla.data.action_mapping import map_to_unified; \
from drifting_vla.training.visualizations import VizLogger; \
print('All imports OK')"

# =============================================================================
# Entrypoint: auto-login WandB + exec user command
# =============================================================================
RUN echo '#!/bin/bash\n\
# Auto-login to WandB if API key is set\n\
if [ -n "$WANDB_API_KEY" ]; then\n\
    python -c "import wandb; wandb.login(key=\"$WANDB_API_KEY\", relogin=True)" 2>/dev/null\n\
fi\n\
exec "$@"' > /entrypoint.sh && chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
CMD ["bash"]

